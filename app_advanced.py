import os
import fitz  # PyMuPDF
from openai import OpenAI
import streamlit as st
import tiktoken

# --- ã‚¢ãƒ—ãƒªã®åŸºæœ¬è¨­å®š ---
st.set_page_config(page_title="è«–æ–‡PDFç¿»è¨³ï¼†è¦ç´„ã‚¢ãƒ—ãƒª (ãƒšãƒ¼ã‚¸å˜ä½å‡¦ç†)", page_icon="ğŸ“„", layout="wide")

# --- OpenAI APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ã™ã‚‹é–¢æ•° (å¤‰æ›´ãªã—) ---
def get_openai_client(api_key):
    if 'openai_client' not in st.session_state or st.session_state.api_key != api_key:
        try:
            st.session_state.openai_client = OpenAI(api_key=api_key)
            st.session_state.api_key = api_key
            st.session_state.openai_client.models.list()
        except Exception as e:
            st.error(f"APIã‚­ãƒ¼ãŒç„¡åŠ¹ã‹ã€æ¥ç¶šã«å•é¡ŒãŒã‚ã‚Šã¾ã™: {e}")
            return None
    return st.session_state.openai_client

# --- ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†é–¢æ•° ---

# (å¤‰æ›´ç‚¹) ãƒšãƒ¼ã‚¸ã”ã¨ã®ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆã‚’è¿”ã™ã‚ˆã†ã«å¤‰æ›´
def extract_text_from_pdf_by_page(uploaded_file) -> list[str]:
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸPDFãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒšãƒ¼ã‚¸ã”ã¨ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã€æ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆã¨ã—ã¦è¿”ã™"""
    try:
        file_bytes = uploaded_file.getvalue()
        doc = fitz.open(stream=file_bytes, filetype="pdf")
        pages_text = [page.get_text("text", sort=True) for page in doc]  # sort=Trueã§èª­ã¿å–ã‚Šé †ã‚’å®‰å®šåŒ–
        doc.close()
        return pages_text
    except Exception as e:
        st.error(f"PDFãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return []

def get_token_count(text, model):
    try: encoding = tiktoken.encoding_for_model(model)
    except KeyError: encoding = tiktoken.get_encoding("cl100k_base")
    return len(encoding.encode(text))

def split_text_into_chunks(text: str, model: str, max_tokens: int = 2000) -> list:
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã«åŸºã¥ã„ã¦ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã™ã‚‹é–¢æ•° (æ”¹å–„ç‰ˆ)"""
    chunks = []
    # ã¾ãšã¯æ”¹è¡Œã§åˆ†å‰²ã‚’è©¦ã¿ã‚‹
    sentences = text.split('\n')
    current_chunk = ""
    for sentence in sentences:
        if get_token_count(current_chunk + sentence + "\n", model) <= max_tokens:
            current_chunk += sentence + "\n"
        else:
            if current_chunk: chunks.append(current_chunk)
            # 1æ–‡è‡ªä½“ãŒé•·ã„å ´åˆã¯ã•ã‚‰ã«å¼·åˆ¶åˆ†å‰²
            while get_token_count(sentence, model) > max_tokens:
                cut_off_point = int(len(sentence) * (max_tokens / get_token_count(sentence, model)))
                chunks.append(sentence[:cut_off_point])
                sentence = sentence[cut_off_point:]
            current_chunk = sentence + "\n"
    if current_chunk: chunks.append(current_chunk)
    return chunks

# (å¤‰æ›´ç‚¹) ãƒšãƒ¼ã‚¸å˜ä½ã§ç¿»è¨³å‡¦ç†ã‚’è¡Œã†æ–°ã—ã„é–¢æ•°
def translate_page_by_page(client: OpenAI, pages_text: list[str], model: str, target_language: str) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆï¼ˆãƒšãƒ¼ã‚¸ã”ã¨ï¼‰ã‚’å—ã‘å–ã‚Šã€ãƒšãƒ¼ã‚¸å˜ä½ã§ç¿»è¨³ã™ã‚‹"""
    if not pages_text: return ""
    
    all_translated_pages = []
    
    # å…¨ä½“ã®é€²æ—ãƒãƒ¼ã®æº–å‚™
    progress_bar = st.progress(0, text="ç¿»è¨³ã®æº–å‚™ã‚’ã—ã¦ã„ã¾ã™...")

    for i, page_text in enumerate(pages_text):
        page_num = i + 1
        progress_text = f"ğŸ“„ ãƒšãƒ¼ã‚¸ {page_num}/{len(pages_text)} ã®ç¿»è¨³ä¸­..."
        progress_bar.progress(i / len(pages_text), text=progress_text)
        
        # 1ãƒšãƒ¼ã‚¸ãŒç©ºãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
        if not page_text.strip():
            all_translated_pages.append(f"--- ãƒšãƒ¼ã‚¸ {page_num} (å†…å®¹ã¯ç©ºã§ã—ãŸ) ---")
            continue

        # 1ãƒšãƒ¼ã‚¸å†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã•ã‚‰ã«ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
        sub_chunks = split_text_into_chunks(page_text, model)
        translated_page_parts = []

        for chunk in sub_chunks:
            try:
                res = client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": f"ã‚ãªãŸã¯ãƒ—ãƒ­ã®ç¿»è¨³å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã€æ„å‘³ã‚’æ­£ç¢ºã«ä¿ã¡ãªãŒã‚‰è‡ªç„¶ãª{target_language}ã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚„æ”¹è¡Œã¯å¯èƒ½ãªé™ã‚Šç¶­æŒã—ã¦ãã ã•ã„ã€‚"},
                        {"role": "user", "content": chunk}
                    ],
                    temperature=0.2
                )
                translated_page_parts.append(res.choices[0].message.content)
            except Exception as e:
                st.error(f"ãƒšãƒ¼ã‚¸ {page_num} ã®ç¿»è¨³ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                translated_page_parts.append(f"--- ã‚¨ãƒ©ãƒ¼ï¼šã“ã®éƒ¨åˆ†ã®ç¿»è¨³ã«å¤±æ•—ã—ã¾ã—ãŸ ---")
        
        # ç¿»è¨³ã•ã‚ŒãŸãƒšãƒ¼ã‚¸ã®ãƒ‘ãƒ¼ãƒ„ã‚’çµåˆ
        translated_page = "\n".join(translated_page_parts)
        all_translated_pages.append(translated_page)

    progress_bar.progress(1.0, text="ç¿»è¨³å®Œäº†ï¼")
    progress_bar.empty()

    # å…¨ã¦ã®ç¿»è¨³æ¸ˆã¿ãƒšãƒ¼ã‚¸ã‚’ã€åŒºåˆ‡ã‚Šç·šã‚’å…¥ã‚Œã¦çµåˆ
    return "\n\n\n".join(f"--- ãƒšãƒ¼ã‚¸ {i+1} ---\n\n{content}" for i, content in enumerate(all_translated_pages))


def summarize_text(client, text, model, custom_prompt):
    # (ã“ã®é–¢æ•°ã¯å‰å›ã®æ®µéšçš„è¦ç´„ã®ã¾ã¾ã§OK)
    if not text: return ""
    try:
        chunks = split_text_into_chunks(text, model, max_tokens=3000)
        intermediate_summaries = []
        st.info(f"ãƒ†ã‚­ã‚¹ãƒˆãŒé•·ã„ãŸã‚ã€{len(chunks)}å€‹ã®ãƒ‘ãƒ¼ãƒˆã«åˆ†ã‘ã¦æ®µéšçš„ã«è¦ç´„ã—ã¾ã™ã€‚")
        bar = st.progress(0, text="æ®µéšçš„è¦ç´„ã‚’é–‹å§‹...")
        for i, chunk in enumerate(chunks):
            prompt = f"ä»¥ä¸‹ã¯è«–æ–‡ã®ä¸€éƒ¨ã§ã™ã€‚ã“ã®éƒ¨åˆ†ã®è¦ç‚¹ã‚’ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚\n\n---\n{chunk}"
            res = client.chat.completions.create(
                model=model, messages=[{"role": "user", "content": prompt}], temperature=0.2)
            intermediate_summaries.append(res.choices[0].message.content)
            bar.progress((i + 1) / len(chunks), text=f"ãƒ‘ãƒ¼ãƒˆ {i+1}/{len(chunks)} ã®è¦ç´„å®Œäº†")
        combined_summary = "\n".join(intermediate_summaries)
        final_prompt = f"ä»¥ä¸‹ã¯è«–æ–‡ã®å„ãƒ‘ãƒ¼ãƒˆã®è¦ç´„ã§ã™ã€‚ã“ã‚Œã‚‰ã®æƒ…å ±ã‚’ä½¿ã£ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŒ‡ç¤ºã«å¾“ã£ãŸæœ€çµ‚çš„ãªè¦ç´„ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŒ‡ç¤º:ã€Œ{custom_prompt}ã€\n\n---\nå„ãƒ‘ãƒ¼ãƒˆã®è¦ç´„:\n{combined_summary}"
        final_res = client.chat.completions.create(
            model=model, messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯å„ªç§€ãªãƒªã‚µãƒ¼ãƒã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚æ¸¡ã•ã‚ŒãŸè¦ç´„ã®æ–­ç‰‡ã‚’çµ±åˆã—ã€æœ€çµ‚çš„ãªå›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"},
                {"role": "user", "content": final_prompt}], temperature=0.5)
        bar.empty()
        return final_res.choices[0].message.content
    except Exception as e:
        st.error(f"è¦ç´„ã‚¨ãƒ©ãƒ¼: {e}"); return ""

# --- Streamlit UIéƒ¨åˆ† (å¤§ããªå¤‰æ›´ã¯ãªã—) ---
st.title("ğŸ“„ è«–æ–‡PDFç¿»è¨³ï¼†è¦ç´„ã‚¢ãƒ—ãƒª (ãƒšãƒ¼ã‚¸å˜ä½å‡¦ç†)")
st.markdown("PDFã‚’1ãƒšãƒ¼ã‚¸ãšã¤ç¿»è¨³ãƒ»è¦ç´„ã—ã€çµæœã‚’Markdownãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚")

# (ã‚µã‚¤ãƒ‰ãƒãƒ¼ã¯å¤‰æ›´ãªã—)
with st.sidebar:
    st.header("âš™ï¸ äº‹å‰è¨­å®š")
    api_key = st.text_input("OpenAI APIã‚­ãƒ¼", type="password")
    model_option = st.selectbox("ä½¿ç”¨ã™ã‚‹GPTãƒ¢ãƒ‡ãƒ«",("gpt-4o", "gpt-4-turbo", "gpt-3.5-turbo"))
    st.header("ğŸ“ å‡¦ç†å†…å®¹ã®é¸æŠ")
    process_option = st.radio("å®Ÿè¡Œã—ãŸã„å‡¦ç†",('ç¿»è¨³ã¨è¦ç´„', 'ç¿»è¨³ã®ã¿', 'è¦ç´„ã®ã¿'), horizontal=True)
    if 'ç¿»è¨³' in process_option:
        target_language = st.selectbox("ç¿»è¨³å…ˆã®è¨€èª",("æ—¥æœ¬èª", "è‹±èª", "ä¸­å›½èª", "éŸ“å›½èª", "ãƒ‰ã‚¤ãƒ„èª", "ãƒ•ãƒ©ãƒ³ã‚¹èª"))
    if 'è¦ç´„' in process_option:
        st.subheader("è¦ç´„ã®æŒ‡ç¤º")
        summarize_source = st.radio("ä½•ã‚’è¦ç´„ã—ã¾ã™ã‹ï¼Ÿ",("å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ", "ç¿»è¨³å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆ"), horizontal=True) if process_option == 'ç¿»è¨³ã¨è¦ç´„' else "å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ"
        custom_prompt = st.text_area("è¦ç´„ã®æŒ‡ç¤ºå†…å®¹","ã“ã®è«–æ–‡ã®ã€ŒèƒŒæ™¯ãƒ»ç›®çš„ã€ã€Œæ‰‹æ³•ã€ã€Œçµæœãƒ»çµè«–ã€ã‚’æ˜ç¢ºã«åˆ†ã‘ã€æ—¥æœ¬èªã§ç®‡æ¡æ›¸ãã§ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚", height=150)

st.header("1. PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
uploaded_file = st.file_uploader("å‡¦ç†ã—ãŸã„PDFã‚’é¸æŠ", type="pdf")

if 'result_generated' not in st.session_state:
    st.session_state.result_generated = False

if uploaded_file:
    if st.button("å®Ÿè¡Œã™ã‚‹", type="primary"):
        if not api_key:
            st.error("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§OpenAI APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚"); st.stop()
        
        client = get_openai_client(api_key)
        if not client: st.stop()

        with st.spinner("PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒšãƒ¼ã‚¸æ¯ã«æŠ½å‡ºä¸­..."):
            # (å¤‰æ›´ç‚¹) ãƒšãƒ¼ã‚¸ã”ã¨ã®ãƒªã‚¹ãƒˆã‚’å—ã‘å–ã‚‹
            pages_text_list = extract_text_from_pdf_by_page(uploaded_file)
        
        if not pages_text_list:
            st.error("PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"); st.stop()
        st.success(f"ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå®Œäº†ã€‚({len(pages_text_list)}ãƒšãƒ¼ã‚¸)")

        translated_text, summary_text = "", ""
        
        # (å¤‰æ›´ç‚¹) æ–°ã—ã„ç¿»è¨³é–¢æ•°ã‚’å‘¼ã³å‡ºã™
        if 'ç¿»è¨³' in process_option:
            translated_text = translate_page_by_page(client, pages_text_list, model_option, target_language)
        
        if 'è¦ç´„' in process_option:
            # (å¤‰æ›´ç‚¹) è¦ç´„ã®ãŸã‚ã€ãƒšãƒ¼ã‚¸ãƒªã‚¹ãƒˆã‚’ä¸€ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆã«çµåˆ
            full_original_text = "\n".join(pages_text_list)
            text_for_summary = translated_text if summarize_source == "ç¿»è¨³å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆ" and translated_text else full_original_text
            summary_text = summarize_text(client, text_for_summary, model_option, custom_prompt)
        
        st.success("å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        
        st.session_state.summary = summary_text
        st.session_state.translation = translated_text
        st.session_state.original = "\n\n--- Page Break ---\n\n".join(pages_text_list) # å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚‚ãƒšãƒ¼ã‚¸åŒºåˆ‡ã‚Šã§è¡¨ç¤º
        st.session_state.filename = uploaded_file.name
        st.session_state.result_generated = True
        st.rerun()

# (çµæœè¡¨ç¤ºéƒ¨åˆ†ã¯å¤‰æ›´ãªã—)
if st.session_state.result_generated:
    st.header("ğŸ“Š å‡¦ç†çµæœ")
    markdown_output = f"# ğŸ“„ ã€Œ{st.session_state.filename}ã€ã®å‡¦ç†çµæœ\n\n"
    if st.session_state.summary:
        markdown_output += f"## æ¦‚è¦\n\n{st.session_state.summary}\n\n---\n\n"
    if st.session_state.translation:
        markdown_output += f"## å…¨æ–‡ç¿»è¨³\n\n{st.session_state.translation}\n\n"
    st.download_button(
        label="âœ… å…¨ã¦ã®æˆæœã‚’Markdownãƒ•ã‚¡ã‚¤ãƒ«ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=markdown_output.encode('utf-8'), # utf-8ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        file_name=f"{os.path.splitext(st.session_state.filename)[0]}_result.md",
        mime="text/markdown",
    )
    tab_titles = []
    if st.session_state.summary: tab_titles.append("è¦ç´„")
    if st.session_state.translation: tab_titles.append("ç¿»è¨³çµæœ")
    tab_titles.append("å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ")
    tabs = st.tabs(tab_titles)
    tab_index = 0
    if st.session_state.summary:
        with tabs[tab_index]: st.markdown(st.session_state.summary)
        tab_index += 1
    if st.session_state.translation:
        with tabs[tab_index]: st.text_area(" ", st.session_state.translation, height=500)
        tab_index += 1
    with tabs[tab_index]:
        st.text_area(" ", st.session_state.original, height=500)